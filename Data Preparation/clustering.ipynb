{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies.\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file created in parsing.ipynb.\n",
    "clusteringDf = pd.read_csv(\"clusteringRaw.csv\",encoding=\"utf8\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy model. To download, run python -m spacy download en_core_web_lg.\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy preprocessing. Since I am using tfidf for clustering, I will remove stop words and lemmatize.\n",
    "texts = []\n",
    "for doc in clusteringDf['document']:\n",
    "    docRaw = nlp(doc)\n",
    "    docProcessed = []\n",
    "    for token in docRaw:\n",
    "        if not token.is_stop and token.pos_ not in [\"PUNCT\",\"PART\",\"CONJ\",\"CCONJ\", \"SPACE\"]:\n",
    "            docProcessed.append(token.lemma_)\n",
    "    texts.append(\" \".join(docProcessed))\n",
    "clusteringDf['document2'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf vectorization with unigrams.\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(clusteringDf['document2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101 clusters was determined by elbow method (intetia vs clusters, find k with greatest negative second derivative) and manually checking the resulting clusters.\n",
    "km = KMeans(n_clusters=101)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input space to get cluster-distance, used to determine match metric.\n",
    "XTrans = km.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe out of clustering results to manually investigate what each cluster represents.\n",
    "cols = []\n",
    "for i in range(101):\n",
    "    cols.append(f\"{i}\")\n",
    "clustersDf = pd.DataFrame(columns=cols)\n",
    "# Find 10 closest documents to each cluster center, extract the category and category id.\n",
    "for i in range(10):\n",
    "    row = []\n",
    "    row.extend(clusteringDf['category'][np.argsort(XTrans,axis=0)[i,:]].tolist())\n",
    "    clustersDf = clustersDf.append(pd.Series(row,index=cols),ignore_index=True)\n",
    "for i in range(10):\n",
    "    row2 = []\n",
    "    row2.extend(clusteringDf['categoryID'][np.argsort(XTrans,axis=0)[i,:]].tolist())\n",
    "    clustersDf = clustersDf.append(pd.Series(row2,index=cols),ignore_index=True)\n",
    "# Find 10 highest coefficient terms of each cluster (see which words represent the cluster).\n",
    "for i in range(10):\n",
    "    row3 = []\n",
    "    row3.extend(np.array(vectorizer.get_feature_names())[np.argsort(km.cluster_centers_,axis=1)[:,-i-1]].tolist())\n",
    "    clustersDf = clustersDf.append(pd.Series(row3,index=cols),ignore_index=True)\n",
    "# Also show the 10 coefficients themselves (higher values = more specialized cluster).\n",
    "for i in range(10):\n",
    "    row4 = []\n",
    "    row4.extend(np.sort(km.cluster_centers_,axis=1)[:,-i-1])\n",
    "    clustersDf = clustersDf.append(pd.Series(row4,index=cols),ignore_index=True)\n",
    "clustersDf.to_csv(\"clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After exporting clustersDf to csv, I manually assigned a category and subcategory to each cluster, creating rows named CATEGORY and SUBCATEGORY.\n",
    "# If the category was hyperspecific (ex: centered around people named George) or seemingly random, I assigned both rows to \"REMOVE\".\n",
    "# After that is done, save the clusters file under clustersNamed and reload. Then remove the affected columns, save under clustersCut and reload.\n",
    "clustersNamedDf = pd.read_csv(\"clustersNamed.csv\",encoding=\"utf8\",index_col=0)\n",
    "clustersCutDf = pd.read_csv(\"clustersCut.csv\",encoding=\"utf8\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"REMOVE\" columns from the cluster-distance space so that it can be sorted and documents can be reassigned to one of the remaining clusters.\n",
    "XTransCut = np.delete(XTrans,[int(col) for col in clustersNamedDf.columns if clustersNamedDf[col][\"CATEGORY\"] == \"REMOVE\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give each document/category its clustering determined category, subcategory, and match metric. Assigned category is saved in the kmeans class, however\n",
    "# it must be manually calculated using XTransCut if it was initially assigned to the \"REMOVE\" category.\n",
    "# I believe the XTransCut and clustersCutDf process can be replaced by using np.argsort and taking the second argument if the first results in \"REMOVE\".\n",
    "categoryList = []\n",
    "subcategoryList = []\n",
    "matchList = []\n",
    "for i in range(len(clusteringDf)):\n",
    "    if clustersNamedDf[str(km.labels_[i])]['CATEGORY'] == \"REMOVE\":\n",
    "        categoryList.append(cutClustersDf[str(np.argmin(XTransCut[i,:]))][\"CATEGORY\"])\n",
    "        subcategoryList.append(cutClustersDf[str(np.argmin(XTransCut[i,:]))][\"SUBCATEGORY\"])\n",
    "        matchList.append((np.max(XTransCut[:,np.argmin(XTransCut[i,:])])-XTransCut[i,np.argmin(XTransCut[i,:])])/(np.max(XTransCut[:,np.argmin(XTransCut[i,:])])-np.min(XTransCut[:,np.argmin(XTransCut[i,:])])))\n",
    "    else:\n",
    "        categoryList.append(clustersNamedDf[str(km.labels_[i])]['CATEGORY'])\n",
    "        subcategoryList.append(clustersNamedDf[str(km.labels_[i])]['SUBCATEGORY'])\n",
    "        matchList.append((np.max(XTrans[:,km.labels_[i]])-XTrans[i,km.labels_[i]])/(np.max(XTrans[:,km.labels_[i]])-np.min(XTrans[:,km.labels_[i]])))\n",
    "clusteringDf['categoryAssigned'] = categoryList\n",
    "clusteringDf['subcategoryAssigned'] = subcategoryList\n",
    "clusteringDf['match'] = matchList\n",
    "clusteringDf.to_csv(\"clusteringResults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clue data file for the next cell.\n",
    "clueDf = pd.read_csv(\"clueRaw.csv\",encoding=\"utf8\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the clustering determined category, subcategory, and match to the clue data file for now.\n",
    "# This is done for convienence for now, but is removed in normalization.ipynb.\n",
    "clueCategoryList = []\n",
    "clueSubcategoryList = []\n",
    "clueMatchList = []\n",
    "for i in clueDf['categoryID'].tolist():\n",
    "    clueCategoryList.append(clusteringDf[clusteringDf[\"categoryID\"]==i][\"categoryAssigned\"].tolist()[0])\n",
    "    clueSubcategoryList.append(clusteringDf[clusteringDf[\"categoryID\"]==i][\"subcategoryAssigned\"].tolist()[0])\n",
    "    clueMatchList.append(clusteringDf[clusteringDf[\"categoryID\"]==i][\"match\"].tolist()[0])\n",
    "clueDf.to_csv(\"clueClustered.csv\")"
   ]
  }
 ]
}